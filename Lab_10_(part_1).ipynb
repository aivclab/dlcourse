{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_10_(part_1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REaV1GblZovs"
      },
      "source": [
        "#Visualizing and understanding ConvNets (part 1)\n",
        "In this Lab we will be experimenting with techniques for visualizing and understanding ConvNets.\n",
        "\n",
        "The first part (part 1) is about simple visualization techniques:\n",
        "\n",
        "- **Visualizing layer activations**\n",
        " - Typically used to monitor training. As training progresses, the layer activations (i.e., feature maps) should become more local and sparse in nature.\n",
        "- **Visualizing filters**\n",
        " - Typically also used to monitor training. As training progresses, the filters should become nice and smooth. Noisy filters indicate that the network is not done training.\n",
        "- **Inspecting fully connected layers with K-NN**\n",
        " - The fully connected (FC) layers are where the magic happens in CNN classifiers. However, it is hard to visualize what these layers have learned. One common approach is run K Nearest Neighbours (K-NN) using the learned feature representation. That is, foor a given test image, compare with nearest neighbours. Do they look similar? In what way are they similar?\n",
        "- **Visualization of FC layers using dimensionality reduction**\n",
        " - Another way to visualize what the FC layers have learned is by means of dimensionality reduction on the learned feature representation. Today, we will be looking at t-SNE.\n",
        "\n",
        "Part 1 of the lab is divided into to sub-parts:\n",
        "\n",
        "- In task 1 to 5 we will demonstrate how to apply the techniques mentioned above on a pre-trained CNN. Specifically, we will be looking at VGG16.\n",
        "\n",
        "- In the final task, you will apply the techniques to monitor training of a custom CNN classifier on the MNIST dataset.\n",
        "\n",
        "Much of part 1 will be a recap of [Lab 2](https://github.com/aivclab/dlcourse/blob/master/Lab2_Solution.ipynb).\n",
        "\n",
        "**Before we start - remember to set runtime to GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSIpfu6wUG1r"
      },
      "source": [
        "**NOTE:** In case you have trouble running Keras/TensorFlow in Colab, try one of the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ01fAojUGCV"
      },
      "source": [
        "# Try this\n",
        "#!pip install --upgrade tensorflow==1.8.0\n",
        "\n",
        "# ... or this\n",
        "#%tensorflow_version 1.x\n",
        "\n",
        "# Check TensorFlow version\n",
        "#import tensorflow as tf\n",
        "#print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKeEwuwPx4Wy"
      },
      "source": [
        "# Task 1 - Inference using pre-trained VGG16\n",
        "This task serves to get us started. We will download a test image and run it through a pre-trained VGG16 network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nULbPEJotF__"
      },
      "source": [
        "### Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OPAkRmltJxD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw0XHKFDtVsM"
      },
      "source": [
        "### Download a test image\n",
        "Lets download an image to work on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir27Fu-DtVNV"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "url = \"https://github.com/aivclab/dlcourse/raw/master/data/cat.jpg\"\n",
        "urllib.request.urlretrieve(url,'/content/gdrive/My Drive/cat.jpg')\n",
        "\n",
        "# Check that the file is in your Drive\n",
        "!ls \"/content/gdrive/My Drive/cat.jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO1eegnUt3ws"
      },
      "source": [
        "### Display test image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfM5qISTtc0N"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read image using OpenCV\n",
        "img_path = '/content/gdrive/My Drive/cat.jpg'\n",
        "img = cv2.cvtColor(cv2.imread(img_path),cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkrSlM7_t8YG"
      },
      "source": [
        "### Set up VGG16\n",
        "First, let's download and instantiate a pre-trained VGG16 network including the top layers (i.e., the classifier):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWbUI5m4t7W9"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.utils import plot_model\n",
        "\n",
        "vgg16_full = VGG16(weights='imagenet',\n",
        "                      include_top=True,\n",
        "                      input_shape=(224, 224, 3))\n",
        "\n",
        "# Uncomment one of the following to print network architecture:\n",
        "#vgg16_full.summary()\n",
        "#plot_model(vgg16_full)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4XJ0stZuWGo"
      },
      "source": [
        "### Preprocess the image\n",
        "Recall that when using a pre-trained model, we have to make sure that we preprocess the input image in the same way as when the model was trained. In the case of VGG16, the correct preprocessing is to subtract a pre-calculated channel-wise mean given by `[103.939, 116.779, 123.68]`. Fortunately, Keras does that for us using the `preprocess_input` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1VWU8vNvC0t"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_data = image.img_to_array(img)\n",
        "img_data = np.expand_dims(img_data, axis=0) # add extra batch dimension\n",
        "img_preprocessed = preprocess_input(img_data.copy())\n",
        "\n",
        "# Range before preprocessing\n",
        "print('np.min(img_data)',np.min(img_data))\n",
        "print('np.max(img_data)',np.max(img_data))\n",
        "\n",
        "# Range after preprocessing\n",
        "print('np.min(img_preprocessed)',np.min(img_preprocessed))\n",
        "print('np.max(img_preprocessed)',np.max(img_preprocessed))\n",
        "\n",
        "# Shapes\n",
        "print('img_preprocessed.shape',img_preprocessed.shape)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(131); plt.imshow(img_data.squeeze()/255); plt.title('Before preproc.');\n",
        "plt.subplot(132); plt.imshow(np.clip(img_preprocessed.squeeze()/128, a_min=0, a_max=1)); plt.title('After preproc. (positive values)');\n",
        "plt.subplot(133); plt.imshow(np.clip(img_preprocessed.squeeze()/128, a_min=-1, a_max=0)+1); plt.title('After preproc. (negative values)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd1DIJpYwAN2"
      },
      "source": [
        "### Classifiy test image\n",
        "Let's run the classifier on the test image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6fJDZB6wnKV"
      },
      "source": [
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "\n",
        "# Predict class probabilities (model pre-trained on ImageNet, therefore 1000 classes)\n",
        "predictions = vgg16_full.predict(img_preprocessed)\n",
        "print('predictions.shape',predictions.shape)\n",
        "\n",
        "# Get top-5 predictions\n",
        "labels = decode_predictions(predictions,top=5)\n",
        "[print(name,prob) for code, name, prob in labels[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vj0XTG4wbKE"
      },
      "source": [
        "Looks like the pre-trained VGG16 model predicts labels  associated with *cat*, as we would expect :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz1n92eEw4p4"
      },
      "source": [
        "# Task 2 - Visualizing layer activations\n",
        "Our goal is to visualize the layer activations or feature maps of all convolution layers. Show stats about convolution layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc0m0ykBw5nd"
      },
      "source": [
        "for i, layer in enumerate(vgg16_full.layers):\n",
        "  \n",
        "  # check for convolutional layer\n",
        "  layer_type = layer.__class__.__name__\n",
        "  \n",
        "  if 'Conv' not in layer_type:\n",
        "    continue\n",
        "  \n",
        "  # get filter weights\n",
        "  layer_name = layer.name\n",
        "  input_shape = layer.input_shape\n",
        "  output_shape = layer.output.shape\n",
        "  filter_shape = layer.get_weights()[0].shape\n",
        "  \n",
        "  print(f\"Layer {i} has name {layer_name}, input shape {input_shape}, filter shape {filter_shape}, and output shape {output_shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fu4o83vxIhW"
      },
      "source": [
        "Note that you can get the layer names from the list above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnH-WNOUyA7r"
      },
      "source": [
        "##Visualizing layer activations\n",
        "Let's visualize the layer activations (i.e., feature maps) of the second conv layer in the first block:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vUYMEehxy38"
      },
      "source": [
        "from keras import Model\n",
        "layer_name = 'block1_conv2'\n",
        "dummy_model = Model(inputs=vgg16_full.input, outputs=vgg16_full.get_layer(layer_name).output)\n",
        "out = dummy_model.predict(img_preprocessed)\n",
        "out = out.squeeze()\n",
        "\n",
        "num_channels = out.shape[-1]\n",
        "height_width = out.shape[0]\n",
        "num_cols = 8\n",
        "num_rows = int(num_channels / num_cols)\n",
        "\n",
        "print('Shape of feature map: ' + str(out.shape))\n",
        "\n",
        "plt.figure(figsize=(16,2*num_rows))\n",
        "for i in range(num_channels):\n",
        "  f = out[:,:,i]\n",
        "  plt.subplot(num_rows,num_cols,i+1)\n",
        "  plt.imshow(f)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"{0:.2f}\".format(f.min()) + \"/\" + \"{0:.2f}\".format(f.max())) # Print min/max intensity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYC7J4RxyZiv"
      },
      "source": [
        "Visualization of layer activations is typically used to monitor the training process. As training progresses, the layer activations (i.e., feature maps) should become more local and sparse in nature. We can also use layer activations to get an understanding of what features the network has learned to look for - at least in the first few layers. As me move deeper and deeper into the network, it becomes increasingly harder to interpret the layer activations.\n",
        "\n",
        "###Questions\n",
        "Using layer activation visualization, answer the following questions:\n",
        "1. What type of features are detected in the `block1_conv2` layer?\n",
        "2. Are the features detected in the `block5_conv3` local and sparse, as we would expect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_ebkd3pegOa"
      },
      "source": [
        "# Task 3 - Visualizing filters\n",
        "Filter visualization is typically also used to monitor training. As training progresses, the filters should become nice and smooth. On the other hand, noisy filters indicate that the network is not done training.\n",
        "\n",
        "Note that each filter can have many channels that in general have to be displayed separately. However, for the first convolution layer, we know that there are only three channels (RGB). So in this case, we can display all channels of the filters simultaneously using an RGB images.\n",
        "\n",
        "Since the gradients during backpropagation tend to be smallest in the first layer of the network, it us usually sufficient to inspect the filters of the first layer in order to verify that the network is done training (again, what we don't want to see is noisy filters).\n",
        "\n",
        "Let's visualize the filters of the first convolution layer of VGG16:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJk1_F2qxpKE"
      },
      "source": [
        "layer_name = 'block1_conv1'\n",
        "layer = vgg16_full.get_layer(layer_name)\n",
        "filters = layer.get_weights()\n",
        "\n",
        "plt.figure(figsize=(8,10))\n",
        "for i in range(64):\n",
        "  f = filters[0][:,:,:,i]\n",
        "  \n",
        "  # Normalize to range 0 ... 1\n",
        "  f -= f.min()\n",
        "  f /= f.max()\n",
        "  \n",
        "  plt.subplot(8,8,i+1)\n",
        "  plt.imshow(f)\n",
        "  plt.axis('off')\n",
        "  plt.title(str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LerIYeD_xff0"
      },
      "source": [
        "###Questions\n",
        "1. Do the filters look smooth and noise-free?\n",
        "2. How would you visualize the filters of the last conv layer (`block5_conv3`)? Hint: There are 512 filters in the last layer, and each filter furthermore has 512 channels. Do you see the challenge in displaying the filters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnwHX9dczdvS"
      },
      "source": [
        "# Task 4 - Inspecting fully connected layers with K-NN\n",
        "In a simple CNN classifier, the objective of fully connected (FC) layers is to take the results of the convolution/pooling process and use them to classify the image into a label.\n",
        "\n",
        "One way to inspect an FC layer is as follows:\n",
        "\n",
        "1. Choose a layer. For VGG, FC layers output a 1000 or 4096-dimensional feature vector for each image.\n",
        "2. Run the network on many images and collect the feature vectors.\n",
        "3. Run K Nearest Neighbours (KNN).\n",
        "4. For a given test image, compare with nearest neighbours. Do they look similar? In what way are they similar?\n",
        "\n",
        "In this task we will be looking af the second FC layer of VGG16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAh_deuhz-rA"
      },
      "source": [
        "### Download more images\n",
        "We need more images from multiple classes. Download csv files with image URLs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC1CPlNKyM2e"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "root = '/content/gdrive/My Drive/' # Don't change this\n",
        "data_dirname = 'data' # Change as you like\n",
        "p = Path(root + data_dirname)\n",
        "p.mkdir(exist_ok=True)\n",
        "\n",
        "classes = ['bird','cat','cow','dog','horse','mouse','pig','sheep']\n",
        "for idx, name in enumerate(classes):\n",
        "  print(name)\n",
        "  folder = name\n",
        "  file = name + '.csv'\n",
        "  url = 'https://raw.githubusercontent.com/klaverhenrik/klaverhenrik.github.io/master/transferlearning/examples/' + name + '.csv'\n",
        "  urllib.request.urlretrieve(url,p/file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg1Gg_2TzNQ1"
      },
      "source": [
        "Run this in case you still have data folders from Lab 2 with overlapping names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uQe8E1HzB30"
      },
      "source": [
        "!rm -rf '/content/gdrive/My Drive/data/bird'\n",
        "!rm -rf '/content/gdrive/My Drive/data/cat'\n",
        "!rm -rf '/content/gdrive/My Drive/data/cow'\n",
        "!rm -rf '/content/gdrive/My Drive/data/dog'\n",
        "!rm -rf '/content/gdrive/My Drive/data/horse'\n",
        "!rm -rf '/content/gdrive/My Drive/data/mouse'\n",
        "!rm -rf '/content/gdrive/My Drive/data/pig'\n",
        "!rm -rf '/content/gdrive/My Drive/data/sheep'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbjp21TwznsC"
      },
      "source": [
        "Download images (this may take a while - ignore any warnings):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQuXCvzqzqqt"
      },
      "source": [
        "from fastai.vision.data import download_images\n",
        "from fastai.vision.data import verify_images\n",
        "\n",
        "max_pics = 400\n",
        "for idx, name in enumerate(classes):\n",
        "  print(name)\n",
        "  folder = name\n",
        "  file = name + '.csv'\n",
        "  dest = p/folder\n",
        "  dest.mkdir(parents=True, exist_ok=True)\n",
        "  download_images(p/file, dest, max_pics=max_pics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO_n0jDlzsd6"
      },
      "source": [
        "Prune and remove non-image files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xgPJxyj0LYM"
      },
      "source": [
        "for c in classes:\n",
        "    print(c)\n",
        "    verify_images(p/c, delete=True, max_size=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gz4UHe_zy7H"
      },
      "source": [
        "### Set up data generator\n",
        "Make data generator that runs through all images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM6y2USm9_NY"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 32\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "generator = datagen.flow_from_directory(str(p), # this is where you specify the path to the main data folder\n",
        "                                        target_size=(224,224),\n",
        "                                        color_mode='rgb',\n",
        "                                        batch_size=batch_size,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbuGRvrmz4sE"
      },
      "source": [
        "Verify that the generator classes and your classes are in the same order:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRspdN9u-S3j"
      },
      "source": [
        "print(generator.class_indices)\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyhMfmz71Qnh"
      },
      "source": [
        "### Modify model to output features from second FC layer\n",
        "Here we are slightly modify the model. How (in what way)?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1kcJrx90UW8"
      },
      "source": [
        "from keras import activations\n",
        "from keras.models import load_model\n",
        "\n",
        "# New model that outputs features of second FC layer (fc2)\n",
        "model = Model(inputs=vgg16_full.input, outputs=vgg16_full.get_layer('fc2').output)\n",
        "\n",
        "features = model.predict(img_preprocessed)\n",
        "print('Feature dimensions: ' + str(features.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP4DCFqJ5Exa"
      },
      "source": [
        "### Run K-NN\n",
        "Extract 4096-dimensional feature vectors from second FC layer and run K Nearest Neighbors:\n",
        "\n",
        "**NOTE:** Code block sometimes crashes. Just run it, until it doesn't crash..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6xP0yjv-zjF"
      },
      "source": [
        "#https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
        "from sklearn import neighbors\n",
        "\n",
        "# Function returns pairs of observations:\n",
        "# - features_raw are the raw images\n",
        "# - features_net are the network feature vectors\n",
        "def extract_features(generator,batch_size,num_batches):\n",
        "    sample_count = batch_size * num_batches\n",
        "    features_raw = np.zeros(shape=(sample_count, 224*224*3))\n",
        "    features_net = np.zeros(shape=(sample_count, 4096))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        print(i)\n",
        "        features_batch = np.reshape(inputs_batch,(batch_size,224*224*3))\n",
        "        features_raw[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "        \n",
        "        features_batch = model.predict(inputs_batch)\n",
        "        features_batch = np.reshape(features_batch,(batch_size,4096))\n",
        "        features_net[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "        \n",
        "        labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch,axis=1)\n",
        "        \n",
        "        i += 1\n",
        "        if i * batch_size >= sample_count:\n",
        "            # Note that since generators yield data indefinitely in a loop,\n",
        "            # we must `break` after every image has been seen once.\n",
        "            break\n",
        "    return features_raw, features_net, labels\n",
        "\n",
        "num_batches = 50 # Somewhat random choise, max number is is np.floor(generator.n / batch_size)\n",
        "features_raw, features_net, labels = extract_features(generator, batch_size, num_batches)\n",
        "\n",
        "# Train K-NN classifier on network features (features_net) with k = 10\n",
        "nbrs = neighbors.NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(features_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYz7CoXP1807"
      },
      "source": [
        "Count number of examples within each class (just a test - numbers should be fairly equal):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L0YzUZ9MzN1"
      },
      "source": [
        "for i in range(len(classes)):\n",
        "  print(i,len(np.where(labels==i)[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ3UBXYZ4EEQ"
      },
      "source": [
        "### Display results\n",
        "Pick a random image and display its 10 closest neighbours:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXSeddEBAf_o"
      },
      "source": [
        "# Pick test image\n",
        "filelist = [x for x in (p/classes[np.random.randint(0,len(classes)-1)]).iterdir() if x.is_file()]\n",
        "img_path = filelist[np.random.randint(0,len(filelist)-1)]\n",
        "print(f\"File path: {img_path}\")\n",
        "\n",
        "# Load and preprocess\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_data = image.img_to_array(img)\n",
        "img_data = np.expand_dims(img_data, axis=0)\n",
        "img_data = preprocess_input(img_data)\n",
        "\n",
        "# Feed test image through network to get feature vector\n",
        "X = model.predict(img_data)\n",
        "X = np.reshape(X,(1,4096)) # Vectorize\n",
        "\n",
        "# Get indicies of 10 nearest neighbours (+ distances)\n",
        "distances, indices = nbrs.kneighbors(X)\n",
        "\n",
        "# Display nearest neighbours\n",
        "print('Input/query image:')\n",
        "plt.figure()\n",
        "im = (img_data[0,:,:,:]+128)/255\n",
        "b,g,r = cv2.split(im)\n",
        "rgb = cv2.merge((r,g,b))\n",
        "plt.imshow(np.clip(rgb,0,1,out=rgb))\n",
        "plt.imshow(rgb)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print('10 nearest neighbours:')\n",
        "plt.figure(figsize=(12,6))\n",
        "for i in range(10):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  im = (np.reshape(features_raw[indices[0][i],:],(224,224,3)) + 128) / 255\n",
        "  b,g,r = cv2.split(im)\n",
        "  rgb = cv2.merge((r,g,b))\n",
        "  plt.imshow(np.clip(rgb,0,1,out=rgb))\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9k7RMrT4TEz"
      },
      "source": [
        "###Questions\n",
        "Recall that our goal is to investigate if the second FC layer has learned useful features. Run the above code block a few times and answer these questions:\n",
        "\n",
        "1. What should we be looking for? In other words, what kind of similarity do you expect to see?\n",
        "2. Are you happy with the results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emC52WVe5pdL"
      },
      "source": [
        "# Task 5 - Visualization of FC layers using dimensionality reduction\n",
        "Another way to visualize the learned feature representation of an FC layer is to reduce the dimensionality of the feature vectors from, say 4096 to 2 dimensions (so that we can plot the features in 2D). Here, we will be using t-Distributed Stochastic Neighbouring Entities (t-SNE).\n",
        "\n",
        "t-SNE minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding‚Äù.\n",
        "As a result, **observations that are close in the high-dimensional space will also be close in the low-dimensional space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j_aLSD87Q_T"
      },
      "source": [
        "### Run t-SNE on extracted feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ExZNc0VC019"
      },
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "# Same as above, but using neural net representation\n",
        "X = features_net\n",
        "y = labels\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ]\n",
        "df = pd.DataFrame(X,columns=feat_cols)\n",
        "df['y'] = y\n",
        "df['y'] = df['y'].apply(lambda i: classes[int(i)])\n",
        "X, y = None, None\n",
        "print('Size of the dataframe: {}'.format(df.shape))\n",
        "\n",
        "rndperm = np.random.permutation(df.shape[0])\n",
        "\n",
        "N = 10000\n",
        "df_subset = df.loc[rndperm[:N],:].copy()\n",
        "data_subset = df_subset[feat_cols].values\n",
        "pca = PCA(n_components=3)\n",
        "pca_result = pca.fit_transform(data_subset)\n",
        "df_subset['pca-one'] = pca_result[:,0]\n",
        "df_subset['pca-two'] = pca_result[:,1] \n",
        "df_subset['pca-three'] = pca_result[:,2]\n",
        "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "\n",
        "time_start = time.time()\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(data_subset)\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
        "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.scatterplot(\n",
        "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "    hue=\"y\",\n",
        "    palette=sns.color_palette(\"bright\", len(classes)),\n",
        "    data=df_subset,\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z7mCto7540F"
      },
      "source": [
        "###Questions\n",
        "1. What can you say about the the topology of the learned representation?\n",
        "2. Are any of the classes clearly separated from the others?\n",
        "3. Which classes are likely confused more often than others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzYgQ1p274EY"
      },
      "source": [
        "# Task 6 - Try it on MNIST\n",
        "Your task is to apply what you have learned above with a custom CNN classifier trained on MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHqe4Ita84Do"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6kfVVtV7JGP"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# Pre-process inputs\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class indices to one-hot vectors\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVRmPFuu88XW"
      },
      "source": [
        "### Set up custom CNN classifier\n",
        "Note that the model is defined using a function call. Use this function if you need to re-initialize the model (i.e., reset all weights to random values).\n",
        "\n",
        "Also note that the layers are given names like `conv1`, `pool1`, etc. You can use these names to access filters and feature maps of different layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcJ-C-aA8BaI"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Activation, Dropout, Conv2D, MaxPooling2D\n",
        "from keras.layers import Input\n",
        "\n",
        "def get_model():\n",
        "  inputs = Input(shape=(28, 28, 1),name='input')\n",
        "\n",
        "  # Encoder (convolutional base)\n",
        "  x = Conv2D(8, kernel_size=(5, 5), activation='relu', name='conv1')(inputs)\n",
        "  x = MaxPooling2D((2, 2),name='pool1')(x)\n",
        "  x = Conv2D(16, kernel_size=(3, 3), activation='relu',name='conv2')(x)\n",
        "  x = MaxPooling2D((2, 2),name='pool2')(x)\n",
        "  x = Conv2D(32, kernel_size=(3, 3), activation='relu',name='conv3')(x)\n",
        "  encoded = Flatten(name='flat')(x)\n",
        "\n",
        "  # Decoder (2 fully connected layers)\n",
        "  x = Dense(64, activation='relu',name='fc1')(encoded)\n",
        "  x = Dropout(0.5,name='drop')(x)\n",
        "  predictions = Dense(num_classes,activation='softmax',name='prediction')(x)\n",
        "\n",
        "  # This creates a callable model that includes the Input layer and the prediction layer\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1GdKjxg_zoI"
      },
      "source": [
        "### Run inference on a random test image\n",
        "**Note** that we haven't trained the model yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-ADK2-jAnFr"
      },
      "source": [
        "N = x_test.shape[0]\n",
        "ix = np.random.randint(N)\n",
        "test_image = x_test[ix]\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "prediction = model.predict(test_image)[0]\n",
        "print('Class probabilities: ' + str(prediction))\n",
        "plt.figure()\n",
        "plt.imshow(test_image.squeeze(),cmap='gray')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0qzoATtB4AJ"
      },
      "source": [
        "### Task\n",
        "1. Display the filters of the first convolution layer (`conv1`).\n",
        "2. Run the test image through the network (like above) and display the activations of the last convolution layer (`conv3`).\n",
        "3. Optionally, repeat for multiple test images.\n",
        "\n",
        "**Tip:** Define functions for both, as it will make the remaining sub-tasks easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFl8VwtMCXOJ"
      },
      "source": [
        "def plot_filters_layer1(model):\n",
        "  # Your code goes here\n",
        "\n",
        "def plot_activations_layer3(model,test_image):\n",
        "  # Your code goes here\n",
        "\n",
        "plot_filters_layer1(model)\n",
        "plot_activations_layer3(model,test_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnL79N15EXAh"
      },
      "source": [
        "### Questions\n",
        "1. How would you characterize the filters? Are they smooth? Are they noisy?\n",
        "2. How would you characterize the layer activations? Are they local and sparse?\n",
        "3. Do you observe the same overall pattern in layer activations for different test images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPIqFIO9Ex0r"
      },
      "source": [
        "### Train the model for 1 epoch\n",
        "Train the model for 1 epoch and display the filters and activations again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HupWF-58Ebs"
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 1\n",
        "\n",
        "# Compile the model before training\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "history = model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EgoFDL0FHhh"
      },
      "source": [
        "Display filters and layer activations again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZf8ABZRFKuu"
      },
      "source": [
        "plot_filters_layer1(model)\n",
        "plot_activations_layer3(model,test_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZceWvdCZFSrC"
      },
      "source": [
        "### Question\n",
        "Now that we have trained the model, the filters and activations are not completely random anymore. Answer the same questions as before:\n",
        "\n",
        "1. How would you characterize the filters? Are they smooth? Are they noisy?\n",
        "2. How would you characterize the layer activations? Are they local and sparse?\n",
        "3. Do you observe the same overall pattern in layer activations for different test images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftgHznP0GT0g"
      },
      "source": [
        "### Train for longer\n",
        "Now, train for 20 epochs (or more if you want):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5DzC6z9FTAM"
      },
      "source": [
        "epochs = 20\n",
        "\n",
        "# Fit model\n",
        "history = model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAL7Kz4_GbpF"
      },
      "source": [
        "Now that we have trained the model for more epochs, the filters/weights should have converged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czB2A4oqFbz_"
      },
      "source": [
        "plot_filters_layer1(model)\n",
        "plot_activations_layer3(model,test_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTF4pgJWGsfb"
      },
      "source": [
        "### Display loss curves\n",
        "Just included for completeness. Verify that the model is not overfitting the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bupboja48HNy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_history(history):\n",
        "  plt.figure(figsize=(20,6))\n",
        "\n",
        "  # summarize history for accuracy\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "show_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gmhbX-9G80-"
      },
      "source": [
        "### Optional sub-task: Try t-SNE on MNIST\n",
        "Try to visualize the first FC layer (`fc1`) of the custom CNN using t-SNE. The MNIST digits should be clearly separated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tTrweWILy-d"
      },
      "source": [
        "# Optional task (difficult) - Implement saliency via occlusion\n",
        "Read about \"Saliency via occlusion\" in the slides of lecture 10. Then implement the technique for VGG16 and test it on the first cat image.\n",
        "\n",
        "I used a window size of 48 pixels and a stride of 4 in my solution.\n",
        "\n",
        "Here is some code to get you started:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxxQYj7q9NHv"
      },
      "source": [
        "img_path = '/content/gdrive/My Drive/cat.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_data = image.img_to_array(img)\n",
        "img_data = np.expand_dims(img_data, axis=0)\n",
        "img_preprocessed = preprocess_input(img_data.copy())\n",
        "\n",
        "predictions = vgg16_full.predict(img_preprocessed)\n",
        "\n",
        "class_index = np.argmax(predictions)\n",
        "print(class_index)\n",
        "\n",
        "prob = predictions[0,class_index]\n",
        "print(prob)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}